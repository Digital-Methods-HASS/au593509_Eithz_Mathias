---
title: "2021 The NATO Secretary Generals Annual Report"
author: "Mathias Eithz"
date: "2022-12-07"
output: html_document
---

Credits: This script is based on Adéla Sobotkovas 'Text mining, sentiment analysis, and visualization' (https://github.com/Digital-Methods-HASS/SentimentAnalysis)
which is inspired by Allison Horst's Advanced Statistics and Data Analysis (https://github.com/allisonhorst/esm244-w2020-lab8).

#START


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

# Note - Before lab:
# Attach tidytext and textdata packages
# Run: get_sentiments(lexicon = "nrc")
get_sentiments(lexicon = "nrc")
11# Should be prompted to install lexicon - choose yes!
# Run: get_sentiments(lexicon = "afinn")
# Should be prompted to install lexicon - choose yes!

```




### Get the 2021 Nato General Secretary's annual report
I will start by retrieving the PDF:
```{r get-document}
NATO_2021_path <- here("data","NATO_2021.pdf")
NATO_2021_text <- pdf_text(NATO_2021_path)
```

### Some wrangling to split up pages into separate lines:


```{r split-lines}
NATO_2021_df <- data.frame(NATO_2021_text) %>% 
  mutate(text_full = str_split(NATO_2021_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 
```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 


### Get the tokens (individual words) in tidy format

Use `tidytext::unnest_tokens()`, to split columns into tokens. We are interested in *words*, so that's the token we'll use:

```{r tokenize}
NATO_2021_tokens <- NATO_2021_df %>% 
  unnest_tokens(word, text_full)
```

Let's count the words!
```{r count-words}
NATO_2021_wc <- NATO_2021_tokens %>% 
  count(word) %>% 
  arrange(-n)
NATO_2021_wc
```


### Remove stop words:

By removing stopwords, the less interesting words like "the" and "to" etc., disappear in favor of a core of more meaningful words, that will emerge afterwards 

We will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
NATO_2021_stop <- NATO_2021_tokens %>% 
  anti_join(stop_words) %>% 
  select(-NATO_2021_text)
```

Now check the counts again: 
```{r count-words2}
NATO_2021_swc <- NATO_2021_stop %>% 
  count(word) %>% 
  arrange(-n)
```

To get rid of all numbers in the `NATO_2021_stop`?
```{r skip-numbers}

NATO_2021_no_numeric <- NATO_2021_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of the top 100 most frequent words in the 2021 Nato General Secretary's annual report

See more: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

```{r wordcloud-prep}
# First I can count the amount of unique words
length(unique(NATO_2021_no_numeric$word))

# The number of unique words is 11.209, but I only want to include the 100 most freqeuent words:
NATO_2021_top100 <- NATO_2021_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
NATO_2021_cloud <- ggplot(data = NATO_2021_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

NATO_2021_cloud
```

In order to arrange the words so the most frequent words appear more visible than the less frequent words use, color and font size are used as highlighters: 
```{r wordcloud-pro}
ggplot(data = NATO_2021_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

Thereby it is easier for the eye to sort out which words are most frequently appearing and therefore interesting.



# Sentiment analysis

For the Sentiment analysis I will be using the following two general-purpose lexica:
  -  AFINN from Finn Årup Nielsen,
  -  nrc from Saif Mohammad and Peter Turney


### Applying the "afinn" lexicon to the 2021 Nato General Secretary's annual report

"afinn": Words ranked from -5 (very negative) to +5 (very positive)
```{r afinn}
get_sentiments(lexicon = "afinn")
# Note: may be prompted to download (yes)

# Let's look at the pretty positive words from the value +3 to +5:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

afinn_pos
```


### Applying the "nrc" lexicon to the 2021 Nato General Secretary's annual report
Nrc: The lexicon includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative.

**Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

```{r nrc}
get_sentiments(lexicon = "nrc")
```


## Sentiment analysis using afinn:

First, bind words in `NATO_2021_stop` to `afinn` lexicon:
```{r bind-afinn}
NATO_2021_afinn <- NATO_2021_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
NATO_2021_afinn_hist <- NATO_2021_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = NATO_2021_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```



```{r afinn-2}
NATO_2021_afinn2 <- NATO_2021_afinn %>% 
  filter(value == -2)
```

It appears that the there is an overweight of more positive words (+1 and +2)

Therefore it is interesting to have a look at which words are actually in this +2 category

```{r afinn-2-more}
# Check the unique 2-score words:
unique(NATO_2021_afinn2$word)

# Count & plot them
NATO_2021_afinn2_n <- NATO_2021_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = NATO_2021_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

```

Summarizing the sentiment of the 2021 Nato General Secretary's annual report using the afinn lexicon:
```{r summarize-afinn}
NATO_2021_summary <- NATO_2021_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

With the mean score being 0.80 an the median score being 1, it can be concluded that the sentiment of the 2021 Nato General Secretary's annual report is 
slightly more positive than negative 



## Using the NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the NATO_2021 non-stopword text with the nrc lexicon: 

```{r bind-bing}
NATO_2021_nrc <- NATO_2021_stop %>% 
  inner_join(get_sentiments("nrc"))
```

By using `anti_join()` it is possible to check which words are excluded:

```{r check-exclusions}
NATO_2021_exclude <- NATO_2021_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(NATO_2021_exclude)

# Count to find the most excluded:
NATO_2021_exclude_n <- NATO_2021_exclude %>% 
  count(word, sort = TRUE)

head(NATO_2021_exclude_n)
```


Now find some counts: 
```{r count-bing}
NATO_2021_nrc_n <- NATO_2021_nrc %>% 
  count(sentiment, sort = TRUE)
```

```{r}

NATO_2021_nrc_n <- NATO_2021_nrc_n %>%
  mutate(percentage = (n/25651)*100)
```



```{r}
# And plot them:

ggplot(data = NATO_2021_nrc_n, aes(x = sentiment, y = percentage)) +
  scale_y_continuous(breaks = seq(0,16,by=1), limits = c(0,16)) +
  geom_col()
```

The sentiment categories 'positive' and 'trust' contains the highest percentages of the 10 sentiment categories. .


It is also possible to show the top 5 words under each of the 10 sentiment categories:
```{r count-nrc}
NATO_2021_nrc_n5 <- NATO_2021_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

NATO_2021_nrc_gg <- ggplot(data = NATO_2021_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
NATO_2021_nrc_gg

# Save it
ggsave(plot = NATO_2021_nrc_gg, 
       here("figures","NATO_2021_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```


## Overall conclusion
Using the two lexica (afinn and nrc) for sentiment analysis of the 2021 Nato General Secretary's annual report I found that: 1) the afinn mean score proved
to be 0,80 suggesting that the overall sentiment of the report is sligthly positive, and 2) using the nrc lexicon the categories "positive" and "trust" turned out to
be the most freguent by far outnumbering the other sentiment categories.






